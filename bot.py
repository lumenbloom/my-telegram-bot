# bot.py
import os
from collections import defaultdict
from telegram import Update
from telegram.ext import (
    ApplicationBuilder,
    CommandHandler,
    MessageHandler,
    filters,
    ContextTypes,
)
from openai import AsyncOpenAI
from fastapi import FastAPI, Request
from fastapi.responses import PlainTextResponse
from contextlib import asynccontextmanager
import asyncio
import time
import tiktoken

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ç¯å¢ƒå˜é‡é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TELEGRAM_TOKEN = os.environ["TELEGRAM_TOKEN"]
LLM_API_KEY    = os.environ["LLM_API_KEY"]
LLM_BASE_URL   = os.environ.get("LLM_BASE_URL", "https://api.deepseek.com/v1")
MODEL_NAME     = os.environ.get("MODEL_NAME", "deepseek-chat")
BOT_PERSONALITY = os.environ.get("BOT_PERSONALITY", "ä½ æ˜¯ä¸€ä¸ªèªæ˜åˆæœ‰è¶£çš„åŠ©æ‰‹ï¼Œè¯·è¯´ä¸­æ–‡ã€‚")

# ğŸ›ï¸ å¯æ§å‚æ•°
MAX_CONTEXT_TOKENS = int(os.environ.get("MAX_CONTEXT_TOKENS", "4000"))
LLM_TEMPERATURE    = float(os.environ.get("LLM_TEMPERATURE", "0.7"))
LLM_MAX_TOKENS     = int(os.environ.get("LLM_MAX_TOKENS", "500"))
MAX_HISTORY_ROUNDS = int(os.environ.get("MAX_HISTORY_ROUNDS", "10"))
CONTEXT_TIMEOUT    = int(os.environ.get("CONTEXT_TIMEOUT", "10"))
STREAM_SWITCH      = os.environ.get("STREAM_SWITCH", "false").lower() in ("true", "1", "yes")  # ä¿®å¤ï¼šæ·»åŠ æµå¼ä¼ è¾“å¼€å…³

PORT               = int(os.environ.get("PORT", 10000))
WEBHOOK_PATH       = "/webhook"
RENDER_EXTERNAL_HOSTNAME = os.environ["RENDER_EXTERNAL_HOSTNAME"]
WEBHOOK_URL        = f"https://{RENDER_EXTERNAL_HOSTNAME}{WEBHOOK_PATH}"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ åˆå§‹åŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¿®æ”¹ user_history ç»“æ„ï¼ŒåŒ…å«æ¶ˆæ¯å†å²å’Œæœ€åè®¿é—®æ—¶é—´
user_history = defaultdict(lambda: {"history": [], "last_access": time.time()})
client = AsyncOpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL, timeout=60.0)
tg_app = None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Token ä¼°ç®—å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")

def estimate_tokens(messages):
    """ä½¿ç”¨ tiktoken å‡†ç¡®è®¡ç®— token æ•°é‡"""
    try:
        text = "".join([msg["content"] for msg in messages])
        return len(encoding.encode(text))
    except Exception:
        text = "".join([msg["content"] for msg in messages])
        return len(text) // 1.5

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ¸…ç†è¿‡æœŸä¸Šä¸‹æ–‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def cleanup_expired_context():
    """æ¸…ç†è¿‡æœŸçš„ç”¨æˆ·ä¸Šä¸‹æ–‡"""
    current_time = time.time()
    expired_users = []
    
    for user_id, user_data in user_history.items():
        if current_time - user_data["last_access"] > CONTEXT_TIMEOUT * 60:
            expired_users.append(user_id)
    
    for user_id in expired_users:
        del user_history[user_id]
        print(f"[æ¸…ç†] ç”¨æˆ· {user_id} çš„ä¸Šä¸‹æ–‡å·²è¿‡æœŸå¹¶è¢«æ¸…ç†")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Handlers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("ä½ å¥½ï¼æˆ‘æ˜¯ç”±å¤§æ¨¡å‹é©±åŠ¨çš„æœºå™¨äººï¼Œéšä¾¿èŠï½\n\nå¯ç”¨å‘½ä»¤:\n/start - å¼€å§‹ä½¿ç”¨\n/reset - é‡ç½®å½“å‰å¯¹è¯\n/clearHistory - æ¸…ç†æ‰€æœ‰å†å²è®°å½•\n/stats - æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯")

async def reset(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """é‡ç½®å½“å‰ç”¨æˆ·çš„å¯¹è¯å†å²"""
    user_id = update.effective_user.id
    if user_id in user_history:
        user_history[user_id]["history"].clear()
        user_history[user_id]["last_access"] = time.time()
    await update.message.reply_text("âœ… å·²é‡ç½®å½“å‰å¯¹è¯å†å²")

async def clear_history(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """æ¸…ç†å½“å‰ç”¨æˆ·çš„æ‰€æœ‰å†å²è®°å½•"""
    user_id = update.effective_user.id
    if user_id in user_history:
        user_history[user_id]["history"].clear()
        user_history[user_id]["last_access"] = time.time()
        await update.message.reply_text("ğŸ—‘ï¸ å·²æ¸…ç†æ‚¨çš„æ‰€æœ‰å¯¹è¯å†å²è®°å½•")
    else:
        await update.message.reply_text("ğŸ˜… æ‚¨è¿˜æ²¡æœ‰ä»»ä½•å¯¹è¯å†å²è®°å½•")

async def stats(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯"""
    user_id = update.effective_user.id
    total_users = len(user_history)
    
    if user_id in user_history:
        user_histories = user_history[user_id]["history"]
        user_messages = len(user_histories)
        user_tokens = estimate_tokens([{"role": "user", "content": msg["content"]} for msg in user_histories if msg["role"] == "user"])
        last_active = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(user_history[user_id]["last_access"]))
        
        stats_text = f"""ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
- æ´»è·ƒç”¨æˆ·æ•° (users): {total_users}
- å½“å‰ç”¨æˆ·æ¶ˆæ¯æ•°: {user_messages}
- å½“å‰ç”¨æˆ·ä¼°ç®— tokens: {user_tokens}
- æœ€åæ´»è·ƒæ—¶é—´: {last_active}"""
    else:
        stats_text = f"""ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
- æ´»è·ƒç”¨æˆ·æ•°: {total_users}
- æ‚¨è¿˜æ²¡æœ‰ä»»ä½•å¯¹è¯å†å²"""

    await update.message.reply_text(stats_text)

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.effective_user.id
    text = update.message.text.strip()
    if not text: return

    # æ›´æ–°æœ€åè®¿é—®æ—¶é—´
    if user_id not in user_history:
        user_history[user_id] = {"history": [], "last_access": time.time()}
    else:
        user_history[user_id]["last_access"] = time.time()
    
    # æ¸…ç†è¿‡æœŸä¸Šä¸‹æ–‡
    cleanup_expired_context()
    
    history = user_history[user_id]["history"]
    history.append({"role": "user", "content": text})
    
    # ğŸ” æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦å’Œè½®æ•°
    while True:
        system_msg = {"role": "system", "content": BOT_PERSONALITY}
        full_context = [system_msg] + history
        tokens = estimate_tokens(full_context)
        
        if tokens <= MAX_CONTEXT_TOKENS:
            messages = full_context
            break
        elif len(history) > MAX_HISTORY_ROUNDS * 2:
            history = history[2:]
        elif len(history) > 1:
            if len(history) >= 2 and history[0]["role"] == "user" and history[1]["role"] == "assistant":
                history = history[2:]
            else:
                history = history[1:]
        else:
            history[-1]["content"] = history[-1]["content"][-500:]
            break

    # ğŸ”„ æ ¹æ® STREAM_SWITCH å†³å®šè°ƒç”¨æ–¹å¼
    if STREAM_SWITCH:
        await handle_stream_response(update, messages, history)
    else:
        await handle_normal_response(update, messages, history)

async def handle_normal_response(update, messages, history):
    """æ™®é€šæ¨¡å¼ï¼šç­‰å¾…å®Œæ•´å›å¤åä¸€æ¬¡æ€§å‘é€"""
    reply = await ask_llm(messages, stream=False)
    if reply:
        history.append({"role": "assistant", "content": reply})
        for i in range(0, len(reply), 4000):
            await update.message.reply_text(reply[i:i+4000], disable_web_page_preview=True)

async def handle_stream_response(update, messages, history):
    """æµå¼æ¨¡å¼ï¼šè¾¹æ¥æ”¶è¾¹å‘é€"""
    assistant_reply = ""
    message_obj = None
    
    try:
        response = await client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages,
            temperature=LLM_TEMPERATURE,
            max_tokens=LLM_MAX_TOKENS,
            stream=True
        )
        
        async for chunk in response:
            if chunk.choices and chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                assistant_reply += content
                
                if not message_obj:
                    message_obj = await update.message.reply_text(content or "...")
                else:
                    try:
                        if len(assistant_reply) % 20 == 0 or len(assistant_reply) < 200:
                            await message_obj.edit_text(assistant_reply[:4000] or "...", disable_web_page_preview=True)
                    except Exception:
                        pass
        
        if assistant_reply:
            history.append({"role": "assistant", "content": assistant_reply})
            try:
                await message_obj.edit_text(assistant_reply[:4000] or "...", disable_web_page_preview=True)
            except:
                pass
                
    except Exception as e:
        error_msg = f"âŒ æµå¼ä¼ è¾“å‡ºé”™: {str(e)[:200]}"
        if not message_obj:
            await update.message.reply_text(error_msg)
        else:
            try:
                await message_obj.edit_text(error_msg)
            except:
                await update.message.reply_text(error_msg)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è°ƒç”¨ LLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def ask_llm(messages, stream=False):
    """ç»Ÿä¸€çš„ LLM è°ƒç”¨æ¥å£"""
    try:
        resp = await client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages,
            temperature=LLM_TEMPERATURE,
            max_tokens=LLM_MAX_TOKENS,
            stream=stream
        )
        
        if stream:
            return resp
        else:
            return resp.choices[0].message.content.strip()
    except Exception as e:
        return f"æŠ±æ­‰ï¼Œå¤§æ¨¡å‹å‡ºé”™äº†ï¼š{str(e)[:200]}"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ åˆå§‹åŒ– Telegram Bot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def init():
    global tg_app
    tg_app = ApplicationBuilder().token(TELEGRAM_TOKEN).build()
    tg_app.add_handler(CommandHandler("start", start))
    tg_app.add_handler(CommandHandler("reset", reset))
    tg_app.add_handler(CommandHandler("clearhistory", clear_history))  # æ–°å¢å‘½ä»¤
    tg_app.add_handler(CommandHandler("clearHistory", clear_history))  # å…¼å®¹å¤§å°å†™
    tg_app.add_handler(CommandHandler("stats", stats))  # æ–°å¢ç»Ÿè®¡å‘½ä»¤
    tg_app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

    await tg_app.initialize()
    await tg_app.bot.set_webhook(WEBHOOK_URL)
    print(f"âœ… Webhook set to: {WEBHOOK_URL}")
    print(f"ğŸ”§ é…ç½®:")
    print(f"   - æœ€å¤§ä¸Šä¸‹æ–‡: {MAX_CONTEXT_TOKENS} tokens")
    print(f"   - æ¸©åº¦: {LLM_TEMPERATURE}")
    print(f"   - æœ€å¤§è¾“å‡º: {LLM_MAX_TOKENS}")
    print(f"   - æœ€å¤§å†å²è½®æ•°: {MAX_HISTORY_ROUNDS}")
    print(f"   - ä¸Šä¸‹æ–‡è¶…æ—¶: {CONTEXT_TIMEOUT} åˆ†é’Ÿ")
    print(f"   - æµå¼ä¼ è¾“: {'âœ…' if STREAM_SWITCH else 'âŒ'}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FastAPI APP Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@asynccontextmanager
async def lifespan(app: FastAPI):
    await init()
    yield

app = FastAPI(lifespan=lifespan)

@app.post(WEBHOOK_PATH)
async def webhook(request: Request):
    json_data = await request.json()
    update = Update.de_json(json_data, tg_app.bot)
    await tg_app.process_update(update)
    return PlainTextResponse("OK")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Main Entry Point â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import uvicorn
    print(f"ğŸš€ Starting bot on port {PORT}...")
    uvicorn.run(app, host="0.0.0.0", port=PORT)