# bot.py
import os
from collections import defaultdict
from telegram import Update
from telegram.ext import (
    ApplicationBuilder,
    CommandHandler,
    MessageHandler,
    filters,
    ContextTypes,
)
from openai import AsyncOpenAI
from fastapi import FastAPI, Request
from fastapi.responses import PlainTextResponse
from contextlib import asynccontextmanager
import asyncio

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ç¯å¢ƒå˜é‡é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TELEGRAM_TOKEN = os.environ["TELEGRAM_TOKEN"]
LLM_API_KEY    = os.environ["LLM_API_KEY"]
LLM_BASE_URL   = os.environ.get("LLM_BASE_URL", "https://api.deepseek.com/v1")
MODEL_NAME     = os.environ.get("MODEL_NAME", "deepseek-chat")
BOT_PERSONALITY = os.environ.get("BOT_PERSONALITY", "ä½ æ˜¯ä¸€ä¸ªèªæ˜åˆæœ‰è¶£çš„åŠ©æ‰‹ï¼Œè¯·è¯´ä¸­æ–‡ã€‚")

# ğŸ›ï¸ å¯æ§å‚æ•°
MAX_CONTEXT_TOKENS = int(os.environ.get("MAX_CONTEXT_TOKENS", "8000"))
LLM_TEMPERATURE    = float(os.environ.get("LLM_TEMPERATURE", "0.7"))
LLM_MAX_TOKENS     = int(os.environ.get("LLM_MAX_TOKENS", "2000"))
STREAM_SWITCH      = os.environ.get("STREAM_SWITCH", "false").lower() == "true"  # âœ… æµå¼å¼€å…³

PORT               = int(os.environ.get("PORT", 10000))
WEBHOOK_PATH       = "/webhook"
RENDER_EXTERNAL_HOSTNAME = os.environ["RENDER_EXTERNAL_HOSTNAME"]
WEBHOOK_URL        = f"https://{RENDER_EXTERNAL_HOSTNAME}{WEBHOOK_PATH}"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ åˆå§‹åŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
user_history = defaultdict(list)
client = AsyncOpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL, timeout=60.0)
tg_app = None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ç®€å•çš„ token ä¼°ç®—å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def estimate_tokens(messages):
    text = "".join([msg["content"] for msg in messages])
    return len(text) // 1.5

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Handlers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("ä½ å¥½ï¼æˆ‘æ˜¯ç”±å¤§æ¨¡å‹é©±åŠ¨çš„æœºå™¨äººï¼Œéšä¾¿èŠï½")

async def reset(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.effective_user.id
    user_history[user_id].clear()
    await update.message.reply_text("âœ… å·²é‡ç½®å¯¹è¯å†å²")

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.effective_user.id
    text = update.message.text.strip()
    if not text: return

    history = user_history[user_id]
    history.append({"role": "user", "content": text})
    
    # ğŸ” æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
    while True:
        system_msg = {"role": "system", "content": BOT_PERSONALITY}
        full_context = [system_msg] + history
        tokens = estimate_tokens(full_context)
        
        if tokens <= MAX_CONTEXT_TOKENS:
            messages = full_context
            break
        elif len(history) > 1:
            if len(history) >= 2 and history[0]["role"] == "user" and history[1]["role"] == "assistant":
                history = history[2:]
            else:
                history = history[1:]
        else:
            history[-1]["content"] = history[-1]["content"][-1000:]
            break

    # ğŸ”„ æ ¹æ® STREAM_SWITCH å†³å®šè°ƒç”¨æ–¹å¼
    if STREAM_SWITCH:
        await handle_stream_response(update, messages, history)
    else:
        await handle_normal_response(update, messages, history)

async def handle_normal_response(update, messages, history):
    """æ™®é€šæ¨¡å¼ï¼šç­‰å¾…å®Œæ•´å›å¤åä¸€æ¬¡æ€§å‘é€"""
    reply = await ask_llm(messages, stream=False)
    if reply:
        history.append({"role": "assistant", "content": reply})
        for i in range(0, len(reply), 4000):
            await update.message.reply_text(reply[i:i+4000], disable_web_page_preview=True)

async def handle_stream_response(update, messages, history):
    """æµå¼æ¨¡å¼ï¼šè¾¹æ¥æ”¶è¾¹å‘é€"""
    assistant_reply = ""
    message_obj = None
    
    try:
        response = await client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages,
            temperature=LLM_TEMPERATURE,
            max_tokens=LLM_MAX_TOKENS,
            stream=True  # âœ… å¼€å¯æµå¼ä¼ è¾“
        )
        
        async for chunk in response:
            if chunk.choices and chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                assistant_reply += content
                
                # å‘é€æµå¼å†…å®¹ï¼ˆTelegram æ¶ˆæ¯ä¸èƒ½å¤ªé¢‘ç¹ï¼‰
                if not message_obj:
                    message_obj = await update.message.reply_text(content or "...")
                else:
                    # ç¼–è¾‘ç°æœ‰æ¶ˆæ¯ï¼ˆæ³¨æ„é¢‘ç‡é™åˆ¶ï¼‰
                    try:
                        if len(assistant_reply) % 20 == 0 or len(assistant_reply) < 200:  # æ§åˆ¶æ›´æ–°é¢‘ç‡
                            await message_obj.edit_text(assistant_reply[:4000] or "...", disable_web_page_preview=True)
                    except Exception:
                        pass  # å¿½ç•¥ç¼–è¾‘é”™è¯¯
        
        # æœ€ç»ˆæ•´ç†å¹¶ä¿å­˜å†å²
        if assistant_reply:
            history.append({"role": "assistant", "content": assistant_reply})
            try:
                await message_obj.edit_text(assistant_reply[:4000] or "...", disable_web_page_preview=True)
            except:
                pass
                
    except Exception as e:
        error_msg = f"âŒ æµå¼ä¼ è¾“å‡ºé”™: {str(e)[:200]}"
        if not message_obj:
            await update.message.reply_text(error_msg)
        else:
            try:
                await message_obj.edit_text(error_msg)
            except:
                await update.message.reply_text(error_msg)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è°ƒç”¨ LLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def ask_llm(messages, stream=False):
    """ç»Ÿä¸€çš„ LLM è°ƒç”¨æ¥å£"""
    try:
        resp = await client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages,
            temperature=LLM_TEMPERATURE,
            max_tokens=LLM_MAX_TOKENS,
            stream=stream
        )
        
        if stream:
            # æµå¼å“åº”åœ¨å¤–é¢å¤„ç†
            return resp
        else:
            return resp.choices[0].message.content.strip()
    except Exception as e:
        return f"æŠ±æ­‰ï¼Œå¤§æ¨¡å‹å‡ºé”™äº†ï¼š{str(e)[:200]}"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ åˆå§‹åŒ– Telegram Bot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def init():
    global tg_app
    tg_app = ApplicationBuilder().token(TELEGRAM_TOKEN).build()
    tg_app.add_handler(CommandHandler("start", start))
    tg_app.add_handler(CommandHandler("reset", reset))
    tg_app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

    await tg_app.initialize()
    await tg_app.bot.set_webhook(WEBHOOK_URL)
    print(f"âœ… Webhook set to: {WEBHOOK_URL}")
    print(f"ğŸ”§ é…ç½®:")
    print(f"   - æœ€å¤§ä¸Šä¸‹æ–‡: {MAX_CONTEXT_TOKENS} tokens")
    print(f"   - æ¸©åº¦: {LLM_TEMPERATURE}")
    print(f"   - æœ€å¤§è¾“å‡º: {LLM_MAX_TOKENS}")
    print(f"   - æµå¼ä¼ è¾“: {'âœ…' if STREAM_SWITCH else 'âŒ'}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FastAPI APP Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@asynccontextmanager
async def lifespan(app: FastAPI):
    await init()
    yield

app = FastAPI(lifespan=lifespan)

@app.post(WEBHOOK_PATH)
async def webhook(request: Request):
    json_data = await request.json()
    update = Update.de_json(json_data, tg_app.bot)
    await tg_app.process_update(update)
    return PlainTextResponse("OK")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Main Entry Point â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import uvicorn
    print(f"ğŸš€ Starting bot on port {PORT}...")
    uvicorn.run(app, host="0.0.0.0", port=PORT)