# bot.py
import os
from collections import defaultdict
from telegram import Update
from telegram.ext import (
    ApplicationBuilder,
    CommandHandler,
    MessageHandler,
    filters,
    ContextTypes,
)
from openai import AsyncOpenAI
from fastapi import FastAPI, Request
from fastapi.responses import PlainTextResponse
from contextlib import asynccontextmanager
import asyncio
import time
import tiktoken

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ç¯å¢ƒå˜é‡é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TELEGRAM_TOKEN = os.environ["TELEGRAM_TOKEN"]
LLM_API_KEY    = os.environ["LLM_API_KEY"]
LLM_BASE_URL   = os.environ.get("LLM_BASE_URL", "https://api.deepseek.com/v1")
MODEL_NAME     = os.environ.get("MODEL_NAME", "deepseek-chat")
BOT_PERSONALITY = os.environ.get("BOT_PERSONALITY", "ä½ æ˜¯ä¸€ä¸ªèªæ˜åˆæœ‰è¶£çš„åŠ©æ‰‹ï¼Œè¯·è¯´ä¸­æ–‡ã€‚")

# ğŸ›ï¸ å¯æ§å‚æ•°
MAX_CONTEXT_TOKENS = int(os.environ.get("MAX_CONTEXT_TOKENS", "4000"))
LLM_TEMPERATURE    = float(os.environ.get("LLM_TEMPERATURE", "0.7"))
LLM_MAX_TOKENS     = int(os.environ.get("LLM_MAX_TOKENS", "500"))  # ä»2000æ”¹ä¸º500
MAX_HISTORY_ROUNDS = int(os.environ.get("MAX_HISTORY_ROUNDS", "10"))  # æ–°å¢ï¼šæœ€å¤§å†å²è½®æ•°
CONTEXT_TIMEOUT    = int(os.environ.get("CONTEXT_TIMEOUT", "10"))  # æ–°å¢ï¼šä¸Šä¸‹æ–‡è¶…æ—¶æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰

PORT               = int(os.environ.get("PORT", 10000))
WEBHOOK_PATH       = "/webhook"
RENDER_EXTERNAL_HOSTNAME = os.environ["RENDER_EXTERNAL_HOSTNAME"]
WEBHOOK_URL        = f"https://{RENDER_EXTERNAL_HOSTNAME}{WEBHOOK_PATH}"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ åˆå§‹åŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¿®æ”¹ user_history ç»“æ„ï¼ŒåŒ…å«æ¶ˆæ¯å†å²å’Œæœ€åè®¿é—®æ—¶é—´
user_history = defaultdict(lambda: {"history": [], "last_access": time.time()})
client = AsyncOpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL, timeout=60.0)
tg_app = None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Token ä¼°ç®—å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä½¿ç”¨ tiktoken è¿›è¡Œæ›´å‡†ç¡®çš„ token è®¡ç®—
encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")  # å¯æ ¹æ®å®é™…æ¨¡å‹è°ƒæ•´

def estimate_tokens(messages):
    """ä½¿ç”¨ tiktoken å‡†ç¡®è®¡ç®— token æ•°é‡"""
    try:
        text = "".join([msg["content"] for msg in messages])
        return len(encoding.encode(text))
    except Exception:
        # å¤‡ç”¨æ–¹æ¡ˆï¼šå­—ç¬¦æ•°ä¼°ç®—
        text = "".join([msg["content"] for msg in messages])
        return len(text) // 1.5

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ¸…ç†è¿‡æœŸä¸Šä¸‹æ–‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def cleanup_expired_context():
    """æ¸…ç†è¿‡æœŸçš„ç”¨æˆ·ä¸Šä¸‹æ–‡"""
    current_time = time.time()
    expired_users = []
    
    for user_id, user_data in user_history.items():
        # æ£€æŸ¥æ—¶é—´è¶…æ—¶
        if current_time - user_data["last_access"] > CONTEXT_TIMEOUT * 60:
            expired_users.append(user_id)
    
    # æ¸…ç†è¿‡æœŸç”¨æˆ·
    for user_id in expired_users:
        del user_history[user_id]
        print(f"[æ¸…ç†] ç”¨æˆ· {user_id} çš„ä¸Šä¸‹æ–‡å·²è¿‡æœŸå¹¶è¢«æ¸…ç†")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Handlers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("ä½ å¥½ï¼æˆ‘æ˜¯ç”±å¤§æ¨¡å‹é©±åŠ¨çš„æœºå™¨äººï¼Œéšä¾¿èŠï½")

async def reset(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.effective_user.id
    if user_id in user_history:
        user_history[user_id]["history"].clear()
        user_history[user_id]["last_access"] = time.time()
    await update.message.reply_text("âœ… å·²é‡ç½®å¯¹è¯å†å²")

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.effective_user.id
    text = update.message.text.strip()
    if not text: return

    # æ›´æ–°æœ€åè®¿é—®æ—¶é—´
    if user_id not in user_history:
        user_history[user_id] = {"history": [], "last_access": time.time()}
    else:
        user_history[user_id]["last_access"] = time.time()
    
    # æ¸…ç†è¿‡æœŸä¸Šä¸‹æ–‡
    cleanup_expired_context()
    
    history = user_history[user_id]["history"]
    history.append({"role": "user", "content": text})
    
    # ğŸ” æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦å’Œè½®æ•°
    while True:
        system_msg = {"role": "system", "content": BOT_PERSONALITY}
        full_context = [system_msg] + history
        tokens = estimate_tokens(full_context)
        
        # æ£€æŸ¥ token æ•°é‡
        if tokens <= MAX_CONTEXT_TOKENS:
            messages = full_context
            break
        # æ£€æŸ¥å†å²è½®æ•°
        elif len(history) > MAX_HISTORY_ROUNDS * 2:  # user + assistant ä¸ºä¸€è½®
            history = history[2:]  # ç§»é™¤æœ€å‰é¢çš„ä¸€è½®å¯¹è¯
        # å‰ªè£å†å²è®°å½•
        elif len(history) > 1:
            if len(history) >= 2 and history[0]["role"] == "user" and history[1]["role"] == "assistant":
                history = history[2:]
            else:
                history = history[1:]
        else:
            # æœ€åæ‰‹æ®µï¼šè£å‰ªå•æ¡æ¶ˆæ¯å†…å®¹
            history[-1]["content"] = history[-1]["content"][-500:]  # å‡å°‘åˆ°500å­—ç¬¦
            break

    # ğŸ”„ æ ¹æ® STREAM_SWITCH å†³å®šè°ƒç”¨æ–¹å¼
    if STREAM_SWITCH:
        await handle_stream_response(update, messages, history)
    else:
        await handle_normal_response(update, messages, history)

async def handle_normal_response(update, messages, history):
    """æ™®é€šæ¨¡å¼ï¼šç­‰å¾…å®Œæ•´å›å¤åä¸€æ¬¡æ€§å‘é€"""
    reply = await ask_llm(messages, stream=False)
    if reply:
        history.append({"role": "assistant", "content": reply})
        for i in range(0, len(reply), 4000):
            await update.message.reply_text(reply[i:i+4000], disable_web_page_preview=True)

async def handle_stream_response(update, messages, history):
    """æµå¼æ¨¡å¼ï¼šè¾¹æ¥æ”¶è¾¹å‘é€"""
    assistant_reply = ""
    message_obj = None
    
    try:
        response = await client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages,
            temperature=LLM_TEMPERATURE,
            max_tokens=LLM_MAX_TOKENS,
            stream=True  # âœ… å¼€å¯æµå¼ä¼ è¾“
        )
        
        async for chunk in response:
            if chunk.choices and chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                assistant_reply += content
                
                # å‘é€æµå¼å†…å®¹ï¼ˆTelegram æ¶ˆæ¯ä¸èƒ½å¤ªé¢‘ç¹ï¼‰
                if not message_obj:
                    message_obj = await update.message.reply_text(content or "...")
                else:
                    # ç¼–è¾‘ç°æœ‰æ¶ˆæ¯ï¼ˆæ³¨æ„é¢‘ç‡é™åˆ¶ï¼‰
                    try:
                        if len(assistant_reply) % 20 == 0 or len(assistant_reply) < 200:  # æ§åˆ¶æ›´æ–°é¢‘ç‡
                            await message_obj.edit_text(assistant_reply[:4000] or "...", disable_web_page_preview=True)
                    except Exception:
                        pass  # å¿½ç•¥ç¼–è¾‘é”™è¯¯
        
        # æœ€ç»ˆæ•´ç†å¹¶ä¿å­˜å†å²
        if assistant_reply:
            history.append({"role": "assistant", "content": assistant_reply})
            try:
                await message_obj.edit_text(assistant_reply[:4000] or "...", disable_web_page_preview=True)
            except:
                pass
                
    except Exception as e:
        error_msg = f"âŒ æµå¼ä¼ è¾“å‡ºé”™: {str(e)[:200]}"
        if not message_obj:
            await update.message.reply_text(error_msg)
        else:
            try:
                await message_obj.edit_text(error_msg)
            except:
                await update.message.reply_text(error_msg)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è°ƒç”¨ LLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def ask_llm(messages, stream=False):
    """ç»Ÿä¸€çš„ LLM è°ƒç”¨æ¥å£"""
    try:
        resp = await client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages,
            temperature=LLM_TEMPERATURE,
            max_tokens=LLM_MAX_TOKENS,
            stream=stream
        )
        
        if stream:
            # æµå¼å“åº”åœ¨å¤–é¢å¤„ç†
            return resp
        else:
            return resp.choices[0].message.content.strip()
    except Exception as e:
        return f"æŠ±æ­‰ï¼Œå¤§æ¨¡å‹å‡ºé”™äº†ï¼š{str(e)[:200]}"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ åˆå§‹åŒ– Telegram Bot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def init():
    global tg_app
    tg_app = ApplicationBuilder().token(TELEGRAM_TOKEN).build()
    tg_app.add_handler(CommandHandler("start", start))
    tg_app.add_handler(CommandHandler("reset", reset))
    tg_app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

    await tg_app.initialize()
    await tg_app.bot.set_webhook(WEBHOOK_URL)
    print(f"âœ… Webhook set to: {WEBHOOK_URL}")
    print(f"ğŸ”§ é…ç½®:")
    print(f"   - æœ€å¤§ä¸Šä¸‹æ–‡: {MAX_CONTEXT_TOKENS} tokens")
    print(f"   - æ¸©åº¦: {LLM_TEMPERATURE}")
    print(f"   - æœ€å¤§è¾“å‡º: {LLM_MAX_TOKENS}")
    print(f"   - æœ€å¤§å†å²è½®æ•°: {MAX_HISTORY_ROUNDS}")
    print(f"   - ä¸Šä¸‹æ–‡è¶…æ—¶: {CONTEXT_TIMEOUT} åˆ†é’Ÿ")
    print(f"   - æµå¼ä¼ è¾“: {'âœ…' if STREAM_SWITCH else 'âŒ'}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FastAPI APP Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@asynccontextmanager
async def lifespan(app: FastAPI):
    await init()
    yield

app = FastAPI(lifespan=lifespan)

@app.post(WEBHOOK_PATH)
async def webhook(request: Request):
    json_data = await request.json()
    update = Update.de_json(json_data, tg_app.bot)
    await tg_app.process_update(update)
    return PlainTextResponse("OK")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Main Entry Point â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import uvicorn
    print(f"ğŸš€ Starting bot on port {PORT}...")
    uvicorn.run(app, host="0.0.0.0", port=PORT)